---
title: "AM01 Final Project Group 4"
author: "Study Group 4 - Harsh Tripathi, Nikolaos Panayotou, Wei Guo, Xenia Huber, Xinyue Zhang"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---

## Ask a manager: Data on annual salary from a survey

### Introduction

The following project focuses on the analysis of a dataset on annual salaries. As part of the analysis we will first perform Exploratory Data Analysis (EDA) to better understand the parameters before we will explore the data further using Data Visualization, Hypothesis Testing and Regression Analysis. The aim is to explain the patterns and differences in different annual salaries.

### Executive Summary

The dataset obtained from the survey  contained a large number of respondents that were predominantly white females with high education levels.Due to the limited observations for countries other that the US we decided to focus our entire analysis only on US-Salary (respondents from the US). In addition, as part of our data cleaning we eliminated some of the columns that were not directly related with annual salary and/or contained too many individual answers (i.e. job title), thus making it hard to conclude something from them. After building a clean and workable dataset, the exploratory data analysis was performed and visualized to get a better understanding of the survey respondents and the data in general. We build the following `hypothesis test`:

- Hypothesis 1 - Difference in Tech Salary among Gender: We observe that the average annual salary for man and woman in the tech industry is significantly difference.
- Hypothesis 2 - Different in Tech-proportion among cities: Proportion of tech-employees in San Francisco is significantly different than in other US cities.
- Hypothesis 3 - Difference in Salary by Race: Confidence intervals of Asians with other races don’t overlap therefore and is significantly different than other races
- Hypothesis 4 - Difference of Tech-Proportion among Race: Proportion of Asians working in Tech (9.39%) is significantly different than proportion of Asians working in other industries (5.38%) as P-value < 2e-16. 


Further, we built `regression models`. First had to decide which variable we were going to use as our dependent variable because we have two choices——\"annual salary\" and \"log annual salary\". After plotting two distribution graphs, we found out that \"log annual salary\" fit into a normal distribution and thus we decided to choose \"log annual salary\" for models generation.

We then took a look at \"age\", \"overall_experience\" and \"field_experience\" three variables because people would become older as they have more years of working experiences. Hence, we run 3 very simple models with each of these as explanatory variable to analyze which of these does the best job of explaining the variability in log of annual salary. All three variables passed t-tests and we finally chose \"field_experience\" as it had the largest R^squared. It also made sense logically since jobs nowadays require people to acquire more specific experiences in order to perform well.

Then we met another problem. We had too many categorical variables in our dataset and if we included most of them without merging some of groups, the model will become both huge and hard to explain. Thus, we did some works on combination for \"field_experience\", \"education_level\", \"industry\" and \"state\". You could find more detailed explanations on how we regroup them later in comments for the regression part. Knowing that we create \"field_exp_grouped\", \"edu_level_grouped\", \"industry_grouped\" and \"state_grouped\" four variables used for models now would be enough.

Thereafter, we built six regression models through iteration and we split our dataset by 0.75:0.25 into train and test groups. The final model we regarded as our best includes six independent variables: \"field_exp_grouped\", \"race_cleaned\", \"gender\", \"edu_level_grouped\", \"industry_grouped\", \"state_grouped\" which has a R-squared value of 0.44 and a RMSE of 0.1068. This model also passed both vif and diagnostic tests so we could extract insights from it. We concluded that people with poor field experiences would get a significant disadvantage on their salary, Hispanic, Latino, or Spanish origin and women were groups paid much lower, people should look for jobs in rich states if they want to have higher annual salary, professional degree gives you a greater salary boost than a PhD, and avoid education and nonprofits industries but Computing or Tech industry instead if you want to get an attractive pay.


```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```

```{r load-libraries, echo=FALSE}
library(googlesheets4)
library(tidyverse)
library(janitor) 
library(skimr)
library(countrycode) # to clean up country names
library(broom)
library(car)
library(ggfortify)
library(stringr)
library(mosaic)
library(ggthemes)
library(GGally)
library(readxl)
library(here)
library(tidyquant)
library(infer)
library(openintro)
```


## Data Cleaning
```{r load_data}

#read local copy
ask_a_manager_2021 <- read_csv(here::here("data", "ask_a_manager_2021.csv")) %>% 
  janitor::clean_names()
  
#skim data to get initial understanding of dataset such as datatypes, missing values etc. 
skimr::skim(ask_a_manager_2021)
```
> The dataset contains a lot of different responses and datatypes. Further, there are also various NA values. Thus we will clean the data in a first step including the following changes:
- Renaming columns
- Deleting columns (i.e. time stamp, other monetary comp)
- Changing character data into categorical (i.e. Age, overall experience, education level)
- Cleaning country name using countrycode()
- Deleting industries with <100 respondants
- Deleting race with <100 respondants
- Regrouping race into only 5 
- Dropping NA values
- Cleaning for outliers in salary


```{r, data_cleaning}
#data cleaning

ask_a_manager_2021_new <- ask_a_manager_2021 %>% 
  #rename long columns
  rename(age=how_old_are_you,overall_experience=overall_years_of_professional_experience,
         field_experience=years_of_experience_in_field,education_level=highest_level_of_education_completed) %>% 
  #remove certain columns
  select(-timestamp,
         -additional_context_on_job_title ,
         -additional_context_on_income,
         -other_monetary_comp,
         -currency_other) %>% 
  #mutate ordered factors
  mutate(age=factor(age,
                    levels=c("under 18","18-24","25-34","35-44","45-54","55-64","65 or over"),
                    labels =c("under 18","18-24","25-34","35-44","45-54","55-64","65 or over") ,
                    ordered = TRUE),
         
         overall_experience=factor(overall_experience,
                                   levels=c("1 year or less","2 - 4 years","5-7 years","8 - 10 years","11 - 20 years","21 - 30 years","31 - 40 years","41 years or more"),
                                   labels =c("less than 1 year","2-4 years","5-7 years","8-10 years","11-20 years","21-30 years","31-40 years","41 years or more") ,ordered = TRUE),
         
         field_experience=factor(field_experience,
                                 levels=c("1 year or less","2 - 4 years","5-7 years","8 - 10 years","11 - 20 years","21 - 30 years","31 - 40 years","41 years or more"),
                                 labels =c("less than 1 year","2-4 years","5-7 years","8-10 years","11-20 years","21-30 years","31-40 years","41 years or more") ,
                                 ordered = TRUE),
         education_level=factor(education_level,levels=c("High School","Some college","College degree","Master's degree","PhD","Professional degree (MD, JD, etc.)"),
                                labels =c("High School","Some college","College degree",
                                          "Master's degree","PhD","Professional degree"),
                                ordered = TRUE),
         gender=case_when(
           gender=="Other or prefer not to answer"|gender=="Prefer not to answer"|is.na(gender)~"Don't know or NA",
           TRUE~gender),state=ifelse(str_detect(state,",")|is.na(state),"NA",state))

#sort out country

# First we need to convert country name to iso3c
ask_a_manager_2021_new$iso3c <-countrycode(
  ask_a_manager_2021_new$country,
  origin = "country.name", destination = "iso3c")
# then change it back to country name
ask_a_manager_2021_new$country <-countrycode(
  ask_a_manager_2021_new$iso3c,
  origin = "iso3c", destination = "country.name")

#create a list of industries with more than 100 respondents
industry_list<-ask_a_manager_2021_new %>% 
        group_by(industry) %>% 
        count(sort=TRUE) %>% 
        filter(n > 100) %>% 
        select(industry) %>% pull()
#filter datafame so that it includes only those industries
ask_a_manager_2021_new<-ask_a_manager_2021_new %>% 
     filter(industry %in% industry_list)

#create a list of races with more than 100 respondents
race_list<-ask_a_manager_2021_new %>% 
  group_by(race) %>% 
  count(sort=TRUE) %>% 
  filter(!is.na(race),n>100) %>% 
  select(race) %>% pull()
ask_a_manager_2021_new<-ask_a_manager_2021_new %>% 
  filter(race %in% race_list)

#drop_na
ask_a_manager_2021_cleaned<-
  ask_a_manager_2021_new %>%  drop_na()


```

> After a first cleaning we skim the new dataset again.

```{r}
skim(ask_a_manager_2021_new)
```

> To get a more comprehensive picture, we will only focus on respondents with a salary between $10,000 and $1,000,000 (removing outliers).

```{r cleaned_data}
#filter
ask_a_manager_2021_cleaned<-
  ask_a_manager_2021_cleaned %>% 
  filter(annual_salary>10000&annual_salary<1000000)

skim(ask_a_manager_2021_cleaned)
```

## Exploratory Data Analysis

> To get a sense of the survey respondents the following graphs explore the respondents origin, their gender, race etc. according to the variables given.

```{r, countries with the most respondents, fig.width=12}
#we will filter for countries with the most respondents
ask_a_manager_2021_cleaned %>% 
  count(country, sort=TRUE) %>% 
  top_n(10) %>% 
  
#plot the counts of each country
  ggplot(aes(x=fct_reorder(country,n,.desc=TRUE),y= n,fill=country))+
  geom_col(fill= "slateblue1")+
  theme_bw()+
  geom_text(aes(label=n),size=3,vjust=-0.25)+
  labs(y="Count", 
       x="Country")+
  guides(fill=FALSE)+
  theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1))+
  NULL
# use countrycode::countryname() to clean country names
```

> As the vast majority of the respondents in this survey is from the USA, the further data analysis will only focus on the US annual salary data. 

```{r, include only us data}
#delete country and rename country value as iso3, filter USA
us_data <-
  ask_a_manager_2021_cleaned %>% 
  select(-country) %>% 
  filter(iso3c== "USA")

us_data <- us_data %>%
  mutate(
    log_salary = log(annual_salary)
    ) %>% 
  drop_na()
```

## Analysis on US-Salary

> In the following the exploratory data analysis is conducted on the different variables to thereafter continue with hypothesis testing and regression models.

```{r, salary_distribution, fig.width = 12}

# How is salary distributed?
ggplot(us_data, aes(x=annual_salary))+
  geom_density()+
 labs(x="Annual Salary", 
       y="Density",
       title="Density Plot of Annual Salary")+
  theme_bw()
```

> Looking at the distribution of Annual Salary, we observe that the distribution is heavily skewed towards the right. This means that most of the respondents have an annual salary around the median, but there are some respondents who have very high annual salaries.

```{r, age_distribution, fig.width = 12}
# age distribution in the survey

us_data %>% 
  count(age) %>% 
  ggplot(aes(x=age,y=n,fill=age)) +
  geom_col()+
  labs(x="Age group", 
       y="Count")+
  geom_text(aes(label=n),size=3,vjust=-0.25)+
  theme_bw()+
  scale_fill_brewer(palette = "Blues")+
  guides(fill=FALSE)
```
> Vast majority of survey respondents are between 25-44.

```{r, most observed industries, fig.width = 12}
# More than 1000 industries can be observed in our dataframe
#we will filter for industries with more than 500 respondents

us_data %>% 
  count(industry, sort=TRUE) %>% 
  mutate(percent = 100* n/sum(n)) %>% 
  filter(n>=500) %>% 
  ggplot(aes(x=fct_reorder(industry,n,.desc = TRUE),y= n,fill=industry))+
  geom_col(fill= "slateblue1")+
  geom_text(aes(label=n),size=3,vjust=-0.25)+
    labs(x="Industry", 
       y="Count",
       title="Industries with >500 repondents")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1))+
  guides(fill=FALSE)
```

> Higher propotion of respondents work in Computing or Tech industry. Interestingly, Nonprofits are the second most frequent industry of the survey.

```{r salary in different industry, fig.width = 10}
top_industries <- us_data %>% 
  group_by(industry) %>% 
  count(sort = TRUE) %>% 
  head(10) %>% 
  select(industry) %>% 
  pull()

us_data %>% 
  filter(industry %in% top_industries) %>%
  mutate(industry = fct_reorder(industry, annual_salary)) %>% 
  ggplot(aes(x = annual_salary, y = industry, 
             fill = industry)) +
  geom_boxplot() +
  #add labels
  labs(title = 'Boxplot of Annual Salary vs Industry',
        x = 'Annual Income',
        y = 'Industry') +
  theme_bw()+
  guides(fill= FALSE)
us_data %>%
  filter(industry %in% top_industries) %>% 
  group_by(industry) %>% 
  ggplot(aes(x = reorder(industry, -annual_salary), y = annual_salary)) +
  geom_violin() +
  geom_boxplot(width=0.1)+
  labs(x="Industry", 
       y="Average Salary",
       title="Distribution of salary for every industry",
       subtitle="Ordered by mean")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))+
  NULL

```
> As the boxplot shows, tech, law and engineering are the top three industries that have highest annual salary.

```{r, education level bar chart, fig.width = 12}
# education
us_data %>% 
  filter(!is.na(education_level)) %>% 
  count(education_level) %>% 
  ggplot(aes(x=education_level,y= n,fill=education_level))+
  geom_col()+
  theme_bw()+
  labs(title= "Frequency of each educational level",
       x= "Education level",
       y="Count")+
  scale_fill_brewer(palette = "Blues")+
  geom_text(aes(label=n),size=3,vjust=-0.25)+
  guides(fill=FALSE)+
  theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1))

```

> The survey respondents also display a very high education level in general.

```{r, race variable manipulation}

#Cleaning the race variable(white was included with other races)
us_data <- us_data %>% 
  mutate(
    race_cleaned = case_when(
      race %in% c('Asian or Asian American', 'Asian or Asian American, White') ~ 'Asian or Asian American',
      race %in% c('Black or African American', 'Black or African American, White') ~ 'Black or African American',
      race %in% c('Hispanic, Latino, or Spanish origin', 'Hispanic, Latino, or Spanish origin, White') ~ 'Hispanic, Latino, or Spanish origin',
      race == "Another option not listed here or prefer not to answer" ~ "Others/Prefer not to say",
      TRUE ~ race
    )
  )
skim(us_data$race_cleaned)
```


```{r, plot the race frequencies, fig.width = 12}
# race
us_data %>% 
  filter(!is.na(race_cleaned)) %>% 
  count(race_cleaned) %>% 
  ggplot(aes(x=fct_reorder(race_cleaned,n,.desc=TRUE),y= n,fill=race_cleaned))+
  geom_col(fill= "slateblue1")+
  theme_bw()+
  labs(title="Race Frequency",
       x= "Race",
       y="Count")+
  geom_text(aes(label=n),size=3,vjust=-0.25)+
  guides(fill=FALSE)+
  theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1))

```

> The vast majority of the respondents are white people. 

```{r, barchart for proffesional experience years, fig.width = 12}

# overall_years_of_professional_experience 
us_data %>% 
  count(overall_experience ) %>% 
  ggplot(aes(x=overall_experience,y = n))+
  geom_col(fill= "slateblue1")+
  labs(x="Years of Professional Experience", 
       y="Count",
       title="Years of Experience Frequency")+
  theme_bw()+
  geom_text(aes(label=n),size=3,vjust=-0.25)+
  theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1))

```

> The largest propotion of respondents have 11-20 years of professional experience.

```{r, barchart for years of professional experience in particular field, fig.width = 12}
# years_of_experience_in_field  
us_data %>% 
  count(field_experience) %>% 
  ggplot(aes(x=field_experience,y = n))+
  geom_col(fill= "slateblue1")+
   labs(x="Years of Experience in Particular Field", 
       y="Count",
       title="Frequency of Field Experience")+
  theme_bw()+
  geom_text(aes(label=n),size=3,vjust=-0.25)+
  theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1))

```

> Years of experience in particular field are more evenly distributed. 


```{r, observe the frequencies of each gender, fig.width = 12}
# gender
us_data %>% 
  count(gender) %>% 
  ggplot(aes(x=fct_reorder(gender,n,.desc=TRUE),y = n,fill=gender))+
  geom_col(fill= "slateblue1")+
  theme_bw()+
  geom_text(aes(label=n),size=3,vjust=-0.25)+
  guides(fill=FALSE)+
  labs(title= "Frequency of each gender",
       x="Gender",
       y="Count")+
  NULL
```

> Survey respondents of America are mostly female.


```{r,compare salaries for each gender, fig.width = 12}

#create table
us_data %>% 
  group_by(gender) %>% 
  summarize(
    mean_salary = mean(annual_salary)
  ) %>% 
#plot barchart  
  ggplot(aes(x = reorder(gender, -mean_salary), y = mean_salary)) +
  geom_col(fill= "slateblue1")+
 geom_text(aes(label=round(mean_salary,0)),size=3,vjust=-0.25)+
   labs(x="Gender", 
       y="Average Salary",
       title="Average Salary for Gender (US)")+
  theme_bw()
```

> Average salary of men is higher than woman and non-binary respondents.

```{r,salary vs experience levels, fig.width = 12}

us_data %>%
  ggplot(aes(x = overall_experience, y = annual_salary, fill=overall_experience)) +
  geom_violin()+
  geom_boxplot(width=0.1)+
  scale_fill_brewer(palette="Blues") + theme_classic()+
   labs(x="Overall Experience", 
       y="Annual Salary",
       title="Annual salary for different experience levels",
       subtitle="Ordered by mean")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))+
  guides(fill=FALSE)+
  NULL

```


```{r, salary for different experience, fig.width = 12}
us_data %>%
  
  ggplot(aes(x = field_experience, y = annual_salary, fill=field_experience)) +
  geom_violin()+
  geom_boxplot(width=0.1)+
  scale_fill_brewer(palette="Blues") + theme_classic()+
  labs(x="Field Experience", 
       y="Annual Salary",
       title="Annual salary for different field experience",
       subtitle="Ordered by mean")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))+
  guides(fill=FALSE)+


  NULL
```


```{r, average salary vs age, fig.width = 12}


us_data %>%
  group_by(age) %>% 
  
  ggplot(aes(x = age, y = annual_salary, fill=age)) +
  geom_violin()+
  scale_fill_brewer(palette="Blues") + theme_classic()+
  geom_boxplot(width=0.1)+
   labs(x="Age", 
       y="Annual Salary",
       title="Distribution of annual salary for age")+
  theme_bw()+
  guides(fill=FALSE)+
  NULL
 

```


```{r, education vs salary, fig.width = 12}

us_data %>%
  ggplot(aes(x = education_level, y = annual_salary,fill=education_level)) +
  geom_violin() +
  scale_fill_brewer(palette="Blues") + theme_classic()+
  geom_boxplot(width=0.1)+
  #geom_text(aes(label=round(mean(annual_salary),0)),size=3,vjust=-0.25)+
  labs(x="Education Level", 
       y="Annual Salary",
       title="Annual salary for different education levels",
       subtitle="mean increases across the plot")+
  theme_bw()+
  guides(fill=FALSE)+
  NULL

```

> To futher investgate why salary of men is higher than woman, we make a graph of the industries they work in. We filter for industries that have more than 500 respondents and surprisingly find that all men work in computing or tech industry. 

```{r, facet wrap for women and men, fig.width = 12}

us_data %>% 
  group_by(gender) %>% 
  count(industry, sort=TRUE) %>% 
  mutate(percent = 100* n/sum(n)) %>% 
  filter(n>=500) %>% 
  ggplot(aes(y=fct_reorder(industry,n,.desc = TRUE),x= n,fill=industry))+
  geom_col()+
  geom_text(aes(label=n),size=3,vjust=-0.25)+
  facet_wrap(~gender)+
    labs(x="Count",
         y="Industry",
         title="Industries with >500 repondents",
         subtitle="Men only work in the tech industy for our data")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1))+
  guides(fill=FALSE)
```


>To investigate gender gaps in salary for various industries, we plot a graph which shows the difference between mean salaries for males and females for top industries -

```{r, gender_gap_industry}
gender <- us_data %>% 
  
  #Filtering data for top industries (having at least 500 respondents) and Men and Women
  filter(industry %in% industry_list, gender %in% c("Man", "Woman")) %>% 
  group_by(industry, gender) %>% 
  
  #Calculating mean salaries after grouping
  summarize(
    mean_salary = mean(annual_salary)
  )

#Pivoting the data to wide format
gender_pivot <- 
  pivot_wider(
    gender,
    names_from = gender,
    values_from = mean_salary
  )

#Calculating the difference between average salaries of males and females
gender_pivot <- gender_pivot %>% 
  mutate(
    diff = Man - Woman,
    indicator = ifelse(diff > 0, "Excess for Males", "Excess for Females")
  )

#Plotting the data
ggplot(gender_pivot, aes(x=reorder(industry, diff), y=diff, label=diff)) +
  geom_bar(stat = 'identity', width = 0.4, aes(fill = indicator)) +
  
  #Filling the bars according to excess means
  scale_fill_manual(labels = c("Excess for Males", "Excess for Females"),
                    values = c("Excess for Males" = "Sky Blue", "Excess for Females" = "Light Pink")) +
  #Adding title and axes labels
  labs(
    title = "Gender Gap in Mean Salaries for Top Industries",
    y = "Difference in Mean Salaries for Males and Females",
    x = "Industry"
  ) +
  theme_bw() +
  #Removing the legend title
  guides(fill=guide_legend(title="")) +
  coord_flip()
```

>We observe that for most of the top industries, males have a higher average salary than females. The difference is largest for Accounting, Banking & Finance, and Computing & Tech industries. For only 3 industries - Agriculture or Forestry, Hospitality & Events and Art& Design, the average salary of females is higher.

>Now, we explore gender gap in salary for states

```{r, gender_gap_states}
states_pay <- us_data %>% 
  
  #Filtering data for Men and Women and removing NAs for states
  filter(state != "NA", gender %in% c("Man", "Woman")) %>% 
  group_by(state, gender) %>% 
  
  #Calculating mean salaries after grouping
  summarize(
    mean_salary = mean(annual_salary)
  )

#Pivoting the data to wide format
states_pivot <- 
  pivot_wider(
    states_pay,
    names_from = gender,
    values_from = mean_salary
  )

#Calculating the difference between average salaries of males and females
states_pivot <- states_pivot %>% 
  mutate(
    diff = Man - Woman,
    indicator = ifelse(diff > 0, "Excess for Males", "Excess for Females")
  )

#Plotting the data
ggplot(states_pivot, aes(x=reorder(state, diff), y=diff, label=diff)) +
  geom_bar(stat = 'identity', width = 0.4, aes(fill = indicator)) +
  
  #Filling the bars according to excess means
  scale_fill_manual(labels = c("Excess for Males", "Excess for Females"),
                    values = c("Excess for Males" = "Sky Blue", "Excess for Females" = "Light Pink")) +
  #Adding title and axes labels
  labs(
    title = "Gender Gap in Mean Salaries for States",
    y = "Difference in Mean Salaries for Males and Females",
    x = "State"
  ) +
  theme_bw() +
  #Removing the legend title
  guides(fill=guide_legend(title="")) +
  coord_flip()
```

>We observe that most of the states have a higher average salary for males, as compared to females. The difference is the largest for South Dakota and Arkansas. For only 3 states - Oklahoma, Wyoming and Maine, the average salary of females is higher.

_Observations for Hypothesis 1 from Exploratory Data Analysis_

>- Observation 1: Mean salary men > Mean salary women
>- Observation 2: Computing & Tech is highest paid industry
>- Observation 3: All male respondents work in Tech

>Following these observations we compare whether the average annual salary for tech industry is different between men and woman.

### Hypothesis 1: 
Null Hypothesis $H_0$ : $\mu_{Men.tech.salary} - \mu_{Female.tech.salary} = 0$

Alternative Hypothesis $H_1$ : $\mu_{Men.tech.salary} - \mu_{Female.tech.salary}\neq 0$

```{r, Salaries in Tech for each gender, height = 0.1}
#compute confidence intervals

us_data_menvswomen <- us_data%>%
  filter(industry == "Computing or Tech") %>% 
  filter(gender %in% c("Man", "Woman")) %>% 
  group_by(gender) %>% 
  summarise(mean_salary = mean(annual_salary),
            median_salary = median(annual_salary),
            sd_salary = sd(annual_salary),
            count = n(),
            # get t-critical value with (n-1) degrees of freedom
            t_critical = qt(0.975, count-1),
            se_salary = sd_salary/sqrt(count),
            margin_of_error = t_critical * se_salary,
            salary_low = mean_salary - margin_of_error,
            salary_high = mean_salary + margin_of_error) %>% 
  arrange(desc(mean_salary))
  us_data_menvswomen
  
 # using welch  t test
us_data_2 <- us_data %>% 
    filter(industry == "Computing or Tech") %>% 
  filter(gender %in% c("Man", "Woman"))


t.test(annual_salary ~ gender , data = us_data_2)

#boxplots
us_data %>% 
  group_by(gender) %>% 
  ggplot(aes(x=gender, y=annual_salary))+
  geom_violin()+
  geom_boxplot(width=0.3)+
  labs(x="Gender", 
       y="Annual Salary",
       title="Annual Salary for Gender (US)")+
  theme_bw()

us_data %>% 
  filter(industry == "Computing or Tech") %>% 
  group_by(gender) %>% 
  ggplot(aes(x=gender, y=annual_salary))+
  geom_violin()+
  geom_boxplot(width=0.3)+
   labs(x="Gender", 
       y="Annual Salary in Tech",
       title="Annual Salary in Tech for Gender (US)")+
  theme_bw()
```

> We use confidence interval and t-test to show the difference. The confidence interval of men is [142345,149903] which is not overlapping with women [116518,121502] and p-value in the t-test is <2e-16. Therefore, we reject the null hypothesis and conclude there is a significant difference in annual salary between men and women who work in tech industry.

_Observations for Hypothesis 2 from Exploratory Data Analysis_

```{r, salaries across USA cities}
#create a table with mean salaries across different US cities
us_data %>% 
  group_by(city) %>% 
  summarise(mean_salary = mean(annual_salary),
            count = n()
         ) %>% 
  arrange(desc(mean_salary)) %>% 
  #display only top 15
 top_n(15) 
```

> San Francisco has the highest mean salary of all US cities

>So far, we have the following observation of San Francisco and tech industry.
>- Observation 1: San Francisco has the highest mean salary of all US cities
>- Observation 2: Mean salary is highest in Computing &Tech

>We want to figure out whether this is because most people in San Francisco tend to work in tech industry compared to other cities in America.


### Hypothesis 2:

Null Hypothesis $H_0$ : $p_{SF.tech} = p_{notSF.tech} $

Alternative Hypothesis $H_1$ : $p_{SF.tech} \neq p_{notSF.tech} $
```{r, SF hypothesis}

#we will examine if San Francisco holds a bigger proportion of tech employees than other cities

#find % of tech employees in SF
us_data %>% 
  filter(city == "San Francisco") %>% 
  count(industry) %>% 
  mutate(percent = n/sum(n)*100,
         total = sum(n)) %>%  
  arrange(desc(n))
#find % of tech employees in US excluding SF  
us_data %>% 
  filter(city != "San Francisco") %>% 
  count(industry) %>% 
  mutate(percent = n/sum(n)*100,
         total = sum(n)) %>% 
  arrange(desc(n))

#two proportion z test
tech<- c(243, 3174)
totals <- c(504, 18990)

res1 <- prop.test(x = tech, n = totals)
res1

```

> We observe that in San Francisco the proportion of tech-employees is 48% vs. 16% in other US cities. Proportion of tech-employees in San Francisco is significantly different than in other US cities as P-value < 2e-16. 

_Observations for Hypothesis 3 from Exploratory Data Analysis_

```{r, race in tech, fig.width = 12}
#plot race in tech industry

us_data %>% 
  filter(industry == "Computing or Tech", race_cleaned != "Others/Prefer not to say") %>%
  group_by(race_cleaned) %>% 
  summarize(
   count = n(),
   mean_salary = mean(annual_salary)
   ) %>% 
  ggplot(aes(x = reorder(race_cleaned, -count), y = count)) +
  geom_col(fill= "slateblue1") +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))+
   labs(x="Race", 
       y="Frequency",
       title="Frequency of each race in Tech")+
  theme_bw()+
  geom_text(aes(label=count),size=3,vjust=-0.25)+
  theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1))

```
```{r, race vs salary, fig.width = 12}
us_data %>% 
  group_by(race_cleaned) %>% 
  filter (race_cleaned != "Others/Prefer not to say" ) %>% 
  summarize(
    mean_salary = mean(annual_salary)
  ) %>% 
  
  ggplot(aes(x = reorder(race_cleaned, -mean_salary), y = mean_salary)) +
  geom_col(fill= "slateblue1") +
  labs(x="Race", 
       y="Average Salary",
       title="Average salary for every race")+
  theme_bw()+
  geom_text(aes(label=round(mean_salary,0)),size=3,vjust=-0.25)+
  theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1))
```

> In the previous two graphs, we see that mean salary of Asian/Asian American is higher than other races. We want to know if this result is significant.

### Hypothesis 3:
Null Hypothesis $H_0$ : $p_{Asian.Salary} = p_{White.Salary} $

Alternative Hypothesis $H_1$ : $p_{Asian.Salary} \neq p_{White.Salary} $

```{r, t test between salaries for different races}

us_data%>%
  group_by(race_cleaned) %>%
  filter (race_cleaned != "Others/Prefer not to say" ) %>% 
  summarise(mean_salary = mean(annual_salary),
            median_salary = median(annual_salary),
            sd_salary = sd(annual_salary),
            count = n(),
            # get t-critical value with (n-1) degrees of freedom
            t_critical = qt(0.975, count-1),
            se_salary = sd_salary/sqrt(count),
            margin_of_error = t_critical * se_salary,
            salary_low_CI = mean_salary - margin_of_error,
            salary_high_CI = mean_salary + margin_of_error) %>% 
  arrange(desc(mean_salary))

 # using welch  t test
us_data_race <- us_data %>% 
  filter(race_cleaned %in% c("White", "Asian or Asian American"))

t.test(annual_salary ~ race_cleaned , data = us_data_race) 
```

> By comparing the confidence interval of annual income between each race, we see there is no overlap between Asian and other races. And the t-test between Asian and White has a p-value <2e-16. Therefore, we conclude annual salary of Asian is significantly higher than other races.

_Observations for Hypothesis 4 from Exploratory Data Analysis_

```{r, race_proportion_tech}
us_data %>% 
  group_by(race_cleaned) %>% 
  filter(race_cleaned != "Others/Prefer not to say") %>% 
  mutate(
    is_tech = case_when(
      industry == "Computing or Tech" ~ 1,
      TRUE ~ 0
    )
  ) %>% 
  summarize(
    percent_tech = mean(is_tech) * 100
  ) %>% 
  arrange(desc(percent_tech)) %>% 
  ggplot(aes(x = reorder(race_cleaned,-percent_tech), y = percent_tech)) +
  geom_col(fill= "slateblue1") +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))+
   labs(x="Race", 
       y="Percent",
       title="Percent of each race working in Tech")+
  theme_bw()+
  geom_text(aes(label=round(percent_tech,1)),size=3,vjust=-0.25)+
  theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1))
```


```{r, asian_american_proportion_cities}
us_data %>% 
  group_by(city) %>% 
  filter(city %in% c("San Francisco", "Seattle","New York","Washington")) %>% 
  mutate(
    is_asian_american = case_when(
      race == "Asian or Asian American" ~ 1,
      TRUE ~ 0
    )
  ) %>% 
  summarize(
    proportion_of_asian_americans = mean(is_asian_american) * 100 
  ) %>% 
  arrange(desc(proportion_of_asian_americans))

```

> We select 5 cities with top annual salary and summarize the propotion of Asian employee there. San Francico has the highest propotion of Asian.
>- Observation 1: Mean salary of Asian/Asian American is higher than other races
>- Observation 2: 18% of respondents in San Francisco are Asians

>Therefore, we want to figure out whether Asian tend to work in tech industries than other fields which leads the result of a large percentage in San Francisco.


### Hypothesis 4:


Null Hypothesis $H_0$ : $p_{Asian.tech} = p_{other.tech} $
Alternative Hypothesis $H_1$ : $p_{Asian.tech} \neq p_{other.tech} $

```{r, proportion of asians in tech vs in other idustries}

#calculate proportions

us_data%>%
  mutate(tech_or_not = case_when(industry == "Computing or Tech"& race_cleaned=="Asian or Asian American" ~ "asian in tech",
                                 industry != "Computing or Tech"& race_cleaned=="Asian or Asian American" ~ "asian in other industry",
                                 industry == "Computing or Tech"& race_cleaned!="Asian or Asian American" ~ "other in tech",
                                 industry != "Computing or Tech"& race_cleaned!="Asian or Asian American" ~ "other in other industry")
  ) %>% 
  count(tech_or_not) 

#two proportion z test
asians<- c(321, 865)
totals <- c(321+3096, 865 + 15212)

res <- prop.test(x = asians, n = totals)
res


```

> The porption of Asian in Tech (9.39%) is significantly different from Asian in other industries (5.38%) as P-value < 2e-16. We can conclude Asians tend to work in tech industries.

## Regression Modeling

>We train a regression model to predict annual salary for respondents in US only. We take a look at the annual salary and the log of annual salary to decide our dependent variable (Y) - 

```{r, distribution_of_salary_and_log_salary, fig.width = 12, fig.height=4}
library(patchwork) #Package to display plots side-by-side

#Histogram for annual salary
annual_salary_plot <- us_data %>% 
  ggplot(aes(x = annual_salary)) +
  geom_histogram() +
  theme_bw() +
  labs(
    title = "Histogram of annual salary",
    y = "Count",
    x = "Annual Salary"
  )

#Histogram for log of annual salary
log_salary_plot <- us_data %>% 
  ggplot(aes(x = log_salary)) +
  geom_histogram() +
  theme_bw() +
  labs(
    title = "Histogram of log of annual salary",
    y = "Count",
    x = "Log of Annual Salary"
  )

#Displaying the plots together
annual_salary_plot + log_salary_plot
```


>We observe that log of annual salary resembles the normal distribution more closely than annual salary and has lesser outliers. Hence, we proceed with the log of annual salary as the dependent variable.


```{r, glimpse_us_data}
#Quick look at the data
glimpse(us_data)
```

>Taking a look at the explanatory variables for the model, we believe that age, overall_experience and field_experience would be highly correlated. Hence, we run 3 very simple models with each of these as explanatory variables to analyze which of these does the best job of explaining the variability in log of annual salary.

>Here are the 3 models to compare age, overall_experience and field_experience -

```{r, age}
#Linear regression model 
model_age <- lm(log_salary ~ age, data = us_data)

#Summary of the model
mosaic::msummary(model_age)
```

```{r, overall_experience}
#Linear regression model 
model_exp <- lm(log_salary ~ overall_experience, data = us_data)

#Summary of the model
mosaic::msummary(model_exp)
```

```{r, field_experience}
#Linear regression model 
model_field_exp <- lm(log_salary ~ field_experience, data = us_data)

#Summary of the model
mosaic::msummary(model_field_exp)
```

>We observe that all 3 models are significant at 95% confidence level as the p-value is less than 0.05. Since the R^squared for separate models with age, overall_experience and field_experience are 5%, 6% and 11% respectively, we conclude that field_experience does the best job of explaining variability in annual salary. It also makes intuitive sense that the number of years an employee has worked in a field would be the best predictor of her salary. Hence, out of these 3 variables, we pick field_experience as our explanatory variable.

>Based on the exploratory data analysis above, we observe a clear relationship between annual salary and field experience, race, gender, education level, industry and state. We will analyze the possibility of including these variables in our model. 

>We observe that all these explanatory variables are categorical. Hence, we try merging some of the categories of these variables, based on the count of people and mean salary in each category. This will help us reduce the sparsity of features and provide a regression model with better fit.

>We observe the count and mean salaries of people in each category of field_experience.


```{r, salary_by_field_exp}
#Count of people and average salary by field_experience
us_data %>%
  group_by(field_experience) %>% 
  
  summarize(
    #Count of people in each category
    count = n(),
    
    #Mean salary in each category
    mean_salary = mean(annual_salary)
  )
```


>We observe that "31-40 years" and 41 years or more" have very low number of respondents, as compared to other categories. Hence, we merge this category with "21-30" years and create a combined category of "21 years or more". Similarly, we combine "less than 1 year" and "2-4 years" categories into "less than 4 years".


```{r, manipulating_field_exp}
#Based on count of people and mean salary in each category, merging categories
us_data <- us_data %>% 
  mutate( 
    #Converting the variable to character data type
    field_exp_character = as.character(field_experience), 
    
    #Merging categories
    field_exp_grouped = case_when(
    field_exp_character %in% c("less than 1 year", "2-4 years") ~ "less than 4 years",
    field_exp_character %in% c("21-30 years", "31-40 years", "41 years or more") ~ "21 years or more",
    TRUE ~ field_exp_character
  )
  )

#Checking the new distribution based on field_experience
us_data %>%
  group_by(field_exp_grouped) %>% 
  summarize(
    count = n(),
    mean_salary = mean(annual_salary)
  )
```

>If we look at the distribution of field_exp_grouped now, we see that the count of people is more uniform as compared to before.

>We combine other categories based on the same logic -


```{r, salary_by_education_level}
#Count of people and average salary by education
us_data %>%
  group_by(education_level) %>% 
  
  summarize(
    #Count of people in each category
    count = n(),
    
    #Mean salary in each category
    mean_salary = mean(annual_salary)
  )
```


>We observe that High School has a low count and the mean salary for High School and Some college education level are close. Hence, we merge these 2 categories into 1. Although we have low count in PhD as well, we do not have a category which has a mean close to that of PhD. Hence, we keep that as a separate category.

```{r, manipulating_education_level}
#Based on count of people and mean salary in each category, merging categories
us_data <- us_data %>% 
  mutate( 
    #Converting the variable to character data type
    edu_level_char = as.character(education_level),
    
    #Merging categories
    edu_level_grouped = case_when(
    edu_level_char %in% c("High School", "Some college") ~ "High School/College",
    TRUE ~ edu_level_char)
  )

#Checking the new distribution based on education_level
us_data %>%
  group_by(edu_level_grouped) %>% 
  summarize(
    count = n(),
    mean_salary = mean(annual_salary)
  )

```

>The distribution by education level looks more uniform after merging some of the categories. Now, we look at annual salary by industry. Since there are a lot of industries, we consider only those in which the number of respondents are at least 500. For the other industries, we will merge them into a single category called "Others".


```{r, salary_by_industry}
industry_list <- us_data %>%
  group_by(industry) %>% 
  summarize(
    #Count of people in each category
    count = n(),
    
    #Mean salary in each category
    mean_salary = mean(annual_salary)
  ) %>% 
  arrange(desc(count))

#Considering only those industries which have at least 500 entries
industry_list <- industry_list %>% 
  filter(count > 500) %>% 
  select(industry) %>% 
  pull()

industry_list
```
>Since there are two industries for education, we check the annual salary be those 2 industries to check if the mean salaries are close enough to be merged.


```{r, education_industry}
us_data %>%
  
  #Filtering for education industries
  filter(industry %in% c("Education (Higher Education)", "Education (Primary/Secondary)")) %>% 
  
  group_by(industry) %>% 
  summarize(
    #Count of people in each category
    count = n(),
    
    #Mean salary in each category
    mean_salary = mean(annual_salary)
  ) %>% 
  arrange(desc(count))
```
>Since the means of both categories are close, we merge them into a single industry called "Education".

```{r, manipulating_industry}
#Based on count of people and mean salary in each category, merging categories
us_data <- us_data %>% 
  mutate( 
    #Merging categories
    industry_grouped = case_when(
    industry %in% c("Education (Higher Education)", "Education (Primary/Secondary)") ~ "Education",
    industry %in% industry_list ~ industry,
    TRUE ~ "Others"
  )
  )

#Checking the new distribution based on industry
us_data %>%
  group_by(industry_grouped) %>% 
  summarize(
    count = n(),
    mean_salary = mean(annual_salary)
  )
```

>These are the 12 industries (including "Others") that we will use in our final model.

>Now, we look at the state variable -


```{r, salary_by_state}
state_salary <- us_data %>%
  group_by(state) %>% 
  summarize(
    #Count of people in each category
    count = n(),
    
    #Mean salary in each category
    mean_salary = mean(annual_salary)
  ) %>% 
  arrange(desc(mean_salary))

state_salary
```

>Since there are a lot of states and the count of some states is very low, including them all would create a massive list of variables. We will club the states into 3 categories of high, medium and low paying states based on the mean salaries.

```{r, state_salary_quartile}
fav_stats(state_salary$mean_salary)
```
>Based approximately on these quartiles of mean salaries, we pick up thresholds to divide the list of states into high, medium and low paying states.

```{r, states_list_based_on_salary}
#Based on count of people and mean salary in each category, categorizing lists based on mean salaries

high_paying_state <- state_salary %>% 
  filter(mean_salary >= 100000) %>% 
  select(state) %>% 
  pull()

moderate_paying_state <- state_salary %>% 
  filter(mean_salary < 100000, mean_salary >= 80000) %>% 
  select(state) %>% 
  pull()

low_paying_state <- state_salary %>% 
  filter(mean_salary < 80000) %>% 
  select(state) %>% 
  pull()
```


```{r, manipulating_states}
#Assigning state category to each row
us_data <- us_data %>% 
  mutate( 
    state_grouped = case_when(
    state %in% high_paying_state ~ "high_paying_state",
    state %in% moderate_paying_state ~ "moderate_paying_state",
    state %in% low_paying_state ~ "low_paying_state")
  )

#Checking the distrubution based on grouped state
us_data %>%
  group_by(state_grouped) %>% 
  summarize(
    count = n(),
    mean_salary = mean(annual_salary)
  )
```

>We have 3 categories in the grouped state variable with a more uniform count than before.

>Before training our model, we divide the data into train and test sets in the proportion of 75:25 respectively. We will train our model on the train sets, use the model to get the predictions on the test set and analyze the error of predictions as compared to the actual values. Analyzing the adjusted R squared and the RMSE on the test data will give us our best model.


```{r, train_test_split}
#Splitting the data into train and test sets

library(rsample) #Package to split data
set.seed(1234)

#Keeping 75% data in the training set
train_test_split <- initial_split(us_data, prop = 0.75)

train_data <- training(train_test_split)
test_data <- testing(train_test_split)
```

>In the first iteration of the model, we use only field experience as the explanatory variable. As mentioned above, we have merged some categories of the field_experience variable on the basis of count and mean salary in each category to form a field_exp_grouped.


```{r, model1}
#Linear regression model 
model1 <- lm(log_salary ~ field_exp_grouped, data = train_data)

#Summary of the model
mosaic::msummary(model1)

#RMSE on the training set
rmse_train <- train_data %>% 
  mutate(predictions = predict(model1, .)) %>% 
  summarise(
    sqrt(sum(predictions - log_salary)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on training set is: ", round(rmse_train, 12)))

#RMSE on the testing set
rmse_test <- test_data %>% 
  mutate(predictions = predict(model1, .)) %>% 
  summarise(
    sqrt(sum(predictions - log_salary)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on testing set is: ", round(rmse_test, 4)))
```

>We observe that all the variables are significant at 95% confidence level as the p-value is less than 0.05. We get an Adjusted R^2 of 11% and the test RMSE is 0.2047. Now, we add "race" to our model.


```{r, model2}
#Linear regression model
model2 <- lm(log_salary ~ field_exp_grouped + race_cleaned, data = train_data)

#Summary of the model
mosaic::msummary(model2)

#RMSE on the training set
rmse_train <- train_data %>% 
  mutate(predictions = predict(model2, .)) %>% 
  summarise(
    sqrt(sum(predictions - log_salary)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on training set is: ", round(rmse_train, 12)))

#RMSE on the testing set
rmse_test <- test_data %>% 
  mutate(predictions = predict(model2, .)) %>% 
  summarise(
    sqrt(sum(predictions - log_salary)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on testing set is: ", round(rmse_test, 4)))
```

On using race as an explanatory variable in addition to age, we see that all the variables are still significant at 95% confidence level. The Adjusted R^2 increases to 12% and the test RMSE in 0.19.

Now, we add gender to our model.


```{r, model3}
#Linear regression model 
model3 <- lm(log_salary ~ field_exp_grouped + race_cleaned + gender, data = train_data)

#Summary of the model
mosaic::msummary(model3)

#RMSE on the training set
rmse_train <- train_data %>% 
  mutate(predictions = predict(model3, .)) %>% 
  summarise(
    sqrt(sum(predictions - log_salary)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on training set is: ", round(rmse_train, 12)))

#RMSE on the testing set
rmse_test <- test_data %>% 
  mutate(predictions = predict(model3, .)) %>% 
  summarise(
    sqrt(sum(predictions - log_salary)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on testing set is: ", round(rmse_test, 4)))
```

>On adding gender to our model, we observe that all variables of field_exp and race are significant. Out of 3 gender variables, 2 are significant at 95% confidence level. The Adjusted R^2 increases to 16% and the test RMSE is 0.18.

>Now, we add education level to our model.

```{r, model4}
#Linear regression model 
model4 <- lm(log_salary ~ field_exp_grouped + race_cleaned + gender + edu_level_grouped, data = train_data)

#Summary of the model
mosaic::msummary(model4)

#RMSE on the training set
rmse_train <- train_data %>% 
  mutate(predictions = predict(model4, .)) %>% 
  summarise(
    sqrt(sum(predictions - log_salary)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on training set is: ", round(rmse_train, 12)))

#RMSE on the testing set
rmse_test <- test_data %>% 
  mutate(predictions = predict(model4, .)) %>% 
  summarise(
    sqrt(sum(predictions - log_salary)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on testing set is: ", round(rmse_test, 4)))
```
>We observe that all education_level variables are significant. The Adjusted R^2 increases to 22% and the test RMSE is 0.22. 
>We notice that the test RMSE increases to 0.22 from 0.18. As the Adjusted R^2 increases but the RMSE also increases, this could mean that we are going towards overfitting. 
However, we still add industry to our model and analyze R^2 and test RMSE -


```{r, model5}
#Linear regression model 
model5 <- lm(log_salary ~ field_exp_grouped + race_cleaned + gender + edu_level_grouped + industry_grouped, data = train_data)

#Summary of the model
mosaic::msummary(model5)

#RMSE on the training set
rmse_train <- train_data %>% 
  mutate(predictions = predict(model5, .)) %>% 
  summarise(
    sqrt(sum(predictions - log_salary)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on training set is: ", round(rmse_train, 12)))

#RMSE on the testing set
rmse_test <- test_data %>% 
  mutate(predictions = predict(model5, .)) %>% 
  summarise(
    sqrt(sum(predictions - log_salary)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on testing set is: ", round(rmse_test, 4)))
```

>We observe that most of the industry variables are significant at 95% confidence level. The R^squared increases to 38% and the test RMSE drops significantly to 0.06.


>Lastly, we add state to our model -


```{r, model6}
#Linear regression model 
model6 <- lm(log_salary ~ field_exp_grouped + race_cleaned + gender + edu_level_grouped + industry_grouped + state_grouped, data = train_data)

#Summary of the model
mosaic::msummary(model6)

#RMSE on the training set
rmse_train <- train_data %>% 
  mutate(predictions = predict(model6, .)) %>% 
  summarise(
    sqrt(sum(predictions - log_salary)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on training set is: ", round(rmse_train, 12)))

#RMSE on the testing set
rmse_test <- test_data %>% 
  mutate(predictions = predict(model6, .)) %>% 
  summarise(
    sqrt(sum(predictions - log_salary)**2 / n())
  ) %>% 
  pull()

cat(paste0("\nRMSE on testing set is: ", round(rmse_test, 4)))
```

>We see that all variables of state are independent. The R^2 increases to 44% and the test RMSE is 0.1068. The regression model is significant at 95% confidence level as the overall p-value is less than 0.05.

```{r, comparison_of_models}
library(huxtable) #Library to make tables for model comparison

#Produce summary table comparing models using huxtable::huxreg()
huxreg(model2, model3, model4, model5, model6,
       statistics = c('#observations' = 'nobs',
                      'Adj. R Squared' = 'adj.r.squared', 
                      'Residual SE' = 'sigma'), 
       bold_signif = 0.05, 
       stars = NULL
) %>% 
  set_caption('Comparison of models')
```

>Based on this comparison, we will proceed with Model6 (with field_exp_grouped, race_cleaned, gender, edu_level_grouped, industry_grouped and state_grouped as explanatory variables). This model has the highest Adjusted R squared (44%) and lowest Residual SE (0.379). This means that this model is able to explain 44% of the variability in log of annual salary. 

>Now, we will calculate VIF and perform residual analysis.

>We check for collinearity between explanatory variables in our model by calculating VIF - 

```{r, vif}
#Calculating the variance inflation factor to check collinearity
car::vif(model6)
```

>On calculating the Variance Inflation Factor (VIF) for the model, we observe that the values are not very high, which means that we do not have a problem of high multi-collinearity in our model.

>We now do residual analysis for our model -


```{r, residual_analysis, fig.width = 10}
library(ggfortify) #Package for residual analysis

#Plotting graphs for residual analysis
autoplot(model6, which = 1:2) +
  #Using black and white theme
  theme_bw()
```

>By performing Residual Analysis, we observe that there is no significant pattern in the Residuals vs Fitted values plot. We can conclude that there is no strong evidence of non-linear relationship between X & Y or non-constant variance. There seems to be no significant pattern in the data that is currently unaccounted for.

>The Q-Q plot has most of the values on the straight line between -2 and +2. Hence, we can conclude that the residuals follow a Normal Distribution.


>We conclude that the log of annual salary can be best explained by the model with field_exp_grouped, race_cleaned, gender, edu_level_grouped, industry_grouped and state_grouped. We are able to explain 44% of the variability in log of annual salaries.